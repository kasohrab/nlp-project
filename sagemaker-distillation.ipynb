{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-specific knowledge distillation for BERT using Transformers & Amazon SageMaker\n",
    "### Text Classification Example using `BERT-Base` as Teacher and `BERT-Tiny` as Student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to our end-to-end task-specific knowledge distilattion Text-Classification example using Transformers, PyTorch & Amazon SageMaker. Distillation is the process of training a small \"student\" to mimic a larger \"teacher\". In this example, we will use [BERT-base](https://huggingface.co/textattack/bert-base-uncased-SST-2) as Teacher and [BERT-Tiny](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) as Student. We will use [Text-Classification](https://huggingface.co/tasks/text-classification) as task-specific knowledge distillation task and the [Stanford Sentiment Treebank v2 (SST-2)](https://paperswithcode.com/dataset/sst) dataset for training.\n",
    "\n",
    "\n",
    "They are two different types of knowledge distillation, the Task-agnostic knowledge distillation (right) and the Task-specific knowledge distillation (left). In this example we are going to use the Task-specific knowledge distillation. \n",
    "\n",
    "![knowledge-distillation](./imgs/knowledge-distillation.png)\n",
    "_Task-specific distillation (left) versus task-agnostic distillation (right). Figure from FastFormers by Y. Kim and H. Awadalla [arXiv:2010.13382]._\n",
    "\n",
    "\n",
    "In Task-specific knowledge distillation a \"second step of distillation\" is used to \"fine-tune\" the model on a given dataset. This idea comes from the [DistilBERT paper](https://arxiv.org/pdf/1910.01108.pdf) where it was shown that a student performed better than simply finetuning the distilled language model:\n",
    "\n",
    "> We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model:79.8 F1 and 70.4 EM, i.e. within 3 points of the full model.\n",
    "\n",
    "If you are more interested in those topics you should defintely read: \n",
    "* [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n",
    "* [FastFormers: Highly Efficient Transformer Models for Natural Language Understanding](https://arxiv.org/abs/2010.13382)\n",
    "\n",
    "Especially the [FastFormers paper](https://arxiv.org/abs/2010.13382) contains great research on what works and doesn't work when using knowledge distillation.\n",
    "\n",
    "---\n",
    "\n",
    "Huge thanks to [Lewis Tunstall](https://www.linkedin.com/in/lewis-tunstall/) and his great [Weeknotes: Distilling distilled transformers](https://lewtun.github.io/blog/weeknotes/nlp/huggingface/transformers/2021/01/17/wknotes-distillation-and-generation.html#fn-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.212.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.34.51)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.21.12)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.2.0)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c0ce72a7a54724957972f82ba9cf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "In this step we will define global configurations and parameters, which are used across the whole end-to-end fine-tuning proccess, e.g. `tokenizer` and `model` we will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::058264334628:role/service-role/AmazonSageMaker-ExecutionRole-20240415T104077\n",
      "sagemaker bucket: sagemaker-us-east-2-058264334628\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The execution role is only available when running a notebook within SageMaker (SageMaker Notebook Instances or Studio). If you run `get_execution_role` in a notebook not on SageMaker, expect a region error._\n",
    "\n",
    "You can comment in the cell below and provide a an IAM Role name with SageMaker permissions to setup your environment out side of SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]=\"region\"\n",
    "\n",
    "# # This ROLE needs to exists with your associated AWS Credentials and needs permission for SageMaker\n",
    "# ROLE_NAME='role-name-of-your-iam-role-with-right-permissions'\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName=ROLE_NAME)['Role']['Arn']\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DistillationTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Normally, when fine-tuning a transformer model using PyTorch you should go with the `Trainer-API`. The [Trainer](https://huggingface.co/docs/transformers/v4.16.1/en/main_classes/trainer#transformers.Trainer) class provides an API for feature-complete training in PyTorch for most standard use cases. \n",
    "\n",
    "In our example we cannot use the `Trainer` out-of-the-box, since we need to pass in two models, the `Teacher` and the `Student` and compute the loss for both. But we can subclass the `Trainer` to create a `DistillationTrainer` which will take care of it and only overwrite the [compute_loss](https://github.com/huggingface/transformers/blob/c4ad38e5ac69e6d96116f39df789a2369dd33c21/src/transformers/trainer.py#L1962) method as well as the `init` method. In addition to this we also need to subclass the `TrainingArguments` to include the our distillation hyperparameters. \n",
    "\n",
    "The [DistillationTrainer](https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/e8d04240d0ebbd7bd0741d196e8902a69a34b414/scripts/train.py#L28) and [DistillationTrainingArguments](https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/e8d04240d0ebbd7bd0741d196e8902a69a34b414/scripts/train.py#L21) are directly integrated into [training script](./scripts/train.py)\n",
    "\n",
    "```python\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        # place teacher on same device as student\n",
    "        self._move_model_to_device(self.teacher, self.model.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss = outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (\n",
    "            loss_function(\n",
    "                F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "                F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1),\n",
    "            )\n",
    "            * (self.args.temperature ** 2)\n",
    "        )\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1.0 - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Creating an Estimator with our Teacher & Student Model\n",
    "\n",
    "In this example, we will use [BERT-base](textattack/bert-base-uncased-SST-2) as Teacher and [BERT-Tiny](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) as Student. Our Teacher is already fine-tuned on our dataset, which makes it easy for us to directly start the distillation training job rather than fine-tuning the teacher first to then distill it afterwards.\n",
    "\n",
    "_**IMPORTANT**: This example will only work with a `Teacher` & `Student` combination where the Tokenizer is creating the same output._\n",
    "\n",
    "Additionally, describes the [FastFormers: Highly Efficient Transformer Models for Natural Language Understanding](https://arxiv.org/abs/2010.13382) paper an additional phenomenon. \n",
    "> In our experiments, we have observed that dis-\n",
    "tilled models do not work well when distilled to a\n",
    "different model type. Therefore, we restricted our\n",
    "setup to avoid distilling RoBERTa model to BERT\n",
    "or vice versa. The major difference between the\n",
    "two model groups is the input token (sub-word) em-\n",
    "bedding. We think that different input embedding\n",
    "spaces result in different output embedding spaces,\n",
    "and knowledge transfer with different spaces does\n",
    "not work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'teacher_id':'allenai/led-large-16384',           \n",
    "    'student_id':'allenai/led-base-16384',           \n",
    "    'dataset_id':'glue',           \n",
    "    'dataset_config':'sst2',             \n",
    "    # distillation parameter\n",
    "    'alpha': 0.5,\n",
    "    'temparature': 4,\n",
    "    # hpo parameter\n",
    "    \"run_hpo\": True,\n",
    "    \"n_trials\": 100,\n",
    "    # push to hub config\n",
    "    'push_to_hub': True,                            \n",
    "    'hub_model_id': 'led-distilled', \n",
    "    'hub_token': HfFolder.get_token()               \n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        \n",
    "    source_dir           = './scripts',       \n",
    "    instance_type        = 'ml.p3.2xlarge',   \n",
    "    instance_count       = 1,                 \n",
    "    role                 = role,              \n",
    "    transformers_version = '4.12',            \n",
    "    pytorch_version      = '1.9',             \n",
    "    py_version           = 'py38',            \n",
    "    hyperparameters      = hyperparameters,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start our Training with Knowledge-Distillation and Hyperparamter optimization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported processor: cpu. You may need to upgrade your SDK version (pip install -U sagemaker) for newer processors. Supported processor(s): gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define a data input dictonary with our uploaded s3 uris\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# setting wait to False to not expose the HF Token\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1335\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;129m@runnable_by_pipeline\u001b[39m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     experiment_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1278\u001b[0m ):\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model using the input training dataset.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    The API calls the Amazon SageMaker CreateTrainingJob API to start\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m        :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m     experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m _TrainingJob\u001b[38;5;241m.\u001b[39mstart_new(\u001b[38;5;28mself\u001b[39m, inputs, experiment_config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:3515\u001b[0m, in \u001b[0;36mFramework._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m   3507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m \n\u001b[1;32m   3510\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;124;03m            constructor if applicable.\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3515\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFramework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_set_debugger_configs()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:879\u001b[0m, in \u001b[0;36mEstimatorBase._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    872\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set any values in the estimator that need to be set before training.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \n\u001b[1;32m    874\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m            constructor if applicable.\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_job_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;66;03m# if output_path was specified we use it otherwise initialize here.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# For Local Mode with local_code=True we don't need an explicit output_path\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:853\u001b[0m, in \u001b[0;36mEstimatorBase._get_or_create_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name\n\u001b[0;32m--> 853\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_base_job_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name_from_base(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_job_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:836\u001b[0m, in \u001b[0;36mEstimatorBase._ensure_base_job_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set ``self.base_job_name`` if it is not set already.\"\"\"\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# honor supplied base_job_name or generate it\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_job_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_job_name\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m get_jumpstart_base_name_if_jumpstart_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_uri)\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m base_name_from_image(\n\u001b[0;32m--> 836\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_image_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, default_base_name\u001b[38;5;241m=\u001b[39mEstimatorBase\u001b[38;5;241m.\u001b[39mJOB_CLASS_NAME\n\u001b[1;32m    837\u001b[0m     )\n\u001b[1;32m    838\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:3687\u001b[0m, in \u001b[0;36mFramework.training_image_uri\u001b[0;34m(self, region)\u001b[0m\n\u001b[1;32m   3672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_image_uri\u001b[39m(\u001b[38;5;28mself\u001b[39m, region\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Docker image to use for training.\u001b[39;00m\n\u001b[1;32m   3674\u001b[0m \n\u001b[1;32m   3675\u001b[0m \u001b[38;5;124;03m    The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3684\u001b[0m \u001b[38;5;124;03m        str: The URI of the Docker image.\u001b[39;00m\n\u001b[1;32m   3685\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage_uris\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_image_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_region_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_framework_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=no-member\u001b[39;49;00m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpy_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=no-member\u001b[39;49;00m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompiler_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorflow_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensorflow_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpytorch_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_instance_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/utilities.py:424\u001b[0m, in \u001b[0;36moverride_pipeline_parameter_var.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(warning_msg_template, arg_name, func_name, \u001b[38;5;28mtype\u001b[39m(value))\n\u001b[1;32m    423\u001b[0m         kwargs[arg_name] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mdefault_value\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:690\u001b[0m, in \u001b[0;36mget_training_image_uri\u001b[0;34m(region, framework, framework_version, py_version, image_uri, distribution, compiler_config, tensorflow_version, pytorch_version, instance_type)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m                     container_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcu118\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpy_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpy_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_framework_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_framework_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_compiler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/utilities.py:424\u001b[0m, in \u001b[0;36moverride_pipeline_parameter_var.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(warning_msg_template, arg_name, func_name, \u001b[38;5;28mtype\u001b[39m(value))\n\u001b[1;32m    423\u001b[0m         kwargs[arg_name] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mdefault_value\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:202\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config, sagemaker_session)\u001b[0m\n\u001b[1;32m    198\u001b[0m hostname \u001b[38;5;241m=\u001b[39m endpoint_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhostname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    200\u001b[0m repo \u001b[38;5;241m=\u001b[39m version_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepository\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 202\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43m_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mversion_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# if container version is available in .json file, utilize that\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainer_version\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:551\u001b[0m, in \u001b[0;36m_processor\u001b[0;34m(instance_type, available_processors, serverless_inference_config)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    547\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid SageMaker instance type: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. For options, see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aws.amazon.com/sagemaker/pricing/instance-types\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(instance_type)\n\u001b[1;32m    549\u001b[0m         )\n\u001b[0;32m--> 551\u001b[0m \u001b[43m_validate_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_processors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:597\u001b[0m, in \u001b[0;36m_validate_arg\u001b[0;34m(arg, available_options, arg_name)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks if the arg is in the available options, and raises a ``ValueError`` if not.\"\"\"\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_options:\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{arg}\u001b[39;00m\u001b[38;5;124m. You may need to upgrade your SDK version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(pip install -U sagemaker) for newer \u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124ms. Supported \u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124m(s): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{options}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg_name\u001b[38;5;241m=\u001b[39marg_name, arg\u001b[38;5;241m=\u001b[39marg, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(available_options))\n\u001b[1;32m    601\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported processor: cpu. You may need to upgrade your SDK version (pip install -U sagemaker) for newer processors. Supported processor(s): gpu."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "# setting wait to False to not expose the HF Token\n",
    "huggingface_estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the Hugging Face Hub intergration with Tensorboard we can inspect our progress directly on the hub, as well as testing checkpoints during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "whoami = HfApi().whoami()\n",
    "username = whoami['name']\n",
    "\n",
    "print(f\"https://huggingface.co/{username}/{hyperparameters['hub_model_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to achieve a `accuracy` of 0.8337, which is a very good result for our model. Our distilled `Tiny-Bert` has 96% less parameters than the teacher `bert-base` and runs ~46.5x faster while preserving over 90% of BERT’s performances as measured on the SST2 dataset.\n",
    "\n",
    "| model | Parameter | Speed-up | Accuracy |\n",
    "|------------|-----------|----------|----------|\n",
    "| BERT-base  | 109M      | 1x       | 93%      |\n",
    "| tiny-BERT  | 4M        | 46.5x    | 83%      |\n",
    "\n",
    "_Note: The [FastFormers paper](https://arxiv.org/abs/2010.13382) uncovered that the biggest boost in performance is observerd when having 6 or more layers in the student. The [google/bert_uncased_L-2_H-128_A-2](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) we used only had 2, which means when changing our student to, e.g. `distilbert-base-uncased` we should better performance in terms of accuracy._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
